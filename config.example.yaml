# ===========================
# Performance Configuration
# ===========================

database_path: /data/media-finder.db

# Scan Workers: Number of concurrent file processors
# - Default: 10 (good for most systems)
# - Increase to 20-30 for fast NVMe/SSD storage
# - Decrease to 5 for slow/network storage to avoid overwhelming the disk
scan_workers: 10

# Scan Buffer Size: File processing queue buffer
# - Default: 100 (good balance)
# - Increase to 500-1000 for very fast storage
# - Decrease to 50 for systems with limited memory
scan_buffer_size: 100

# Disk Scan Workers: Number of concurrent workers for disk location scanning
# - Default: 5 (good for most systems)
# - Increase to 10-15 for very fast storage
# - Only used if disks are configured for cross-disk duplicate detection
disk_scan_workers: 5

# API Timeout: Timeout for external service API calls
# - Default: 5m (recommended for large libraries)
# - Note: Actual timeout is 2x this value (10m) to handle pagination/retries
# - Increase to 10m if you have very large libraries (100K+ items per service)
# - Decrease to 2m for smaller, faster services
api_timeout: 5m

# Server Configuration
server_port: 8080
cors_allowed_origin: "http://localhost:8080"  # Set to "*" for development

# Statistics Cache TTL: How long to cache expensive statistics queries
# - Default: 30s
# - Increase to 60s-120s for large databases (100K+ files)
# - Decrease to 10s for smaller databases if you need real-time stats
stats_cache_ttl: 30s

# Scan Log Retention: How long to keep scan logs in the database
# - Default: 30 (keep logs for 30 days)
# - Set to 0 to keep logs indefinitely
# - Set to -1 to disable persistent logging entirely
# - Older logs are automatically cleaned up during scheduled maintenance
scan_log_retention_days: 30

# Database Connection Pool Settings
# - Defaults are optimized for SQLite with WAL mode
# - max_open_conns: Maximum concurrent database connections
# - max_idle_conns: Idle connections kept in pool
# - conn_max_lifetime: Maximum connection lifetime before recycling
# - cache_size: SQLite cache size in KB (1000000 ≈ 1GB, higher = faster queries)
db_max_open_conns: 25
db_max_idle_conns: 5
db_conn_max_lifetime: 5m
db_cache_size: 1000000  # 1GB cache for 128GB RAM systems

# Media-finder's own path mappings (service = media-finder's container path, local = host filesystem path)
local_path_mappings:
  - service: /media
    local: /mnt/user/data/media
  - service: /downloads
    local: /mnt/user/data/downloads/torrents

# Each service's path mappings (service = external service's path, local = media-finder's path)
service_path_mappings:
  plex:
    - service: /media
      local: /mnt/user/data/media
  sonarr:
    - service: /tv
      local: /mnt/user/data/media/tv
    - service: /downloads
      local: /mnt/user/data/downloads/torrents
  radarr:
    - service: /movies
      local: /mnt/user/data/media/movies
    - service: /downloads
      local: /mnt/user/data/downloads/torrents
  qbittorrent:
    - service: /downloads
      local: /mnt/user/data/downloads/torrents
  stash:
    - service: /stash
      local: /mnt/user/data/media/stash

services:
  plex:
    url: http://plex:32400
    token: YOUR_PLEX_TOKEN_HERE
    # Optional: Specify library keys to scan (empty = scan all libraries)
    # Use the config page UI to fetch and select specific libraries
    libraries: []
  sonarr:
    url: http://sonarr:8989
    api_key: YOUR_SONARR_API_KEY_HERE
  radarr:
    url: http://radarr:7878
    api_key: YOUR_RADARR_API_KEY_HERE
  qbittorrent:
    url: http://qbittorrent:8080
    username: admin
    password: adminpass
    # Optional: Use qui proxy instead of direct connection
    # qui_proxy_url: http://qui:7476/proxy/YOUR_PROXY_KEY_HERE
  stash:
    url: http://stash:9999
    api_key: YOUR_STASH_API_KEY_HERE

# Paths to scan (as seen by media-finder container)
scan_paths:
  - /media
  - /downloads

# ===========================
# Disk Configuration
# ===========================
# Configure individual disk mounts for cross-disk duplicate detection
# IMPORTANT: These paths must be mounted in your docker-compose.yml
# Example docker-compose.yml volume configuration:
#   volumes:
#     - /mnt/disk1:/disk1:rw
#     - /mnt/disk2:/disk2:rw
#     - /mnt/disk3:/disk3:rw
disks:
  - name: "Disk 1"
    mount_path: "/disk1"
  - name: "Disk 2"
    mount_path: "/disk2"
  - name: "Disk 3"
    mount_path: "/disk3"
  # Add more disks as needed
  # - name: "Disk 4"
  #   mount_path: "/disk4"

# ===========================
# Duplicate Detection Configuration
# ===========================
duplicate_detection:
  # Enable duplicate detection features
  enabled: true

  # Hash algorithm to use for file comparison
  # Options: "sha256" (default, widely supported) or "blake3" (faster, requires additional implementation)
  hash_algorithm: "sha256"

  # Hash mode determines hashing strategy
  # Options:
  #   - "quick_manual": Hash first+last 1MB only, manually verify duplicates (recommended, fast screening)
  #   - "quick_auto": Hash first+last 1MB, auto full-hash any quick duplicates (balanced)
  #   - "progressive": Start at 1MB, incrementally upgrade duplicates (1MB→10MB→100MB→1GB→10GB→Full)
  #   - "full": Hash entire file content (slowest, most accurate, use for critical data)
  # Default: "quick_manual"
  hash_mode: "quick_manual"

  # Hash ordering strategy - determines which files to hash first
  # Options:
  #   - "smallest_first": Process smallest files first (fast initial progress, recommended)
  #   - "largest_first": Process largest files first (prioritize space savings potential)
  #   - "random": Random order (avoid disk seek patterns)
  #   - "by_disk": Group files by disk (process one disk at a time)
  #   - "by_duplicate_probability": Group same-size files together (find duplicates faster)
  #   - "by_modification_time_newest": Process recently modified files first
  #   - "by_modification_time_oldest": Process older files first
  #   - "db_order": No specific ordering (database insertion order)
  # Default: "smallest_first"
  hash_order: "smallest_first"

  # Number of parallel hash calculation workers
  # - Default: 4 (good balance for most systems)
  # - Increase to 8-12 for fast NVMe storage
  # - Decrease to 2-3 for slower HDDs to avoid disk thrashing
  hash_workers: 4

  # Buffer size for reading files during hashing
  # - Format: "1MB", "4MB", "8MB", "16MB"
  # - Default: "4MB" (optimal for most large media files)
  # - Larger buffers = faster hashing for large files (fewer syscalls)
  # - Range: 512KB - 16MB recommended
  # - With 128GB RAM, even 16MB × 10 workers = only 160MB
  hash_buffer_size: "4MB"

  # Only hash files larger than this size (in bytes)
  # - Default: 10485760 (10 MB)
  # - Smaller files are generally unique and not worth the hashing overhead
  # - Set to 0 to hash all files
  min_file_size: 10485760

  # Don't hash files larger than this size (in bytes)
  # - Default: 0 (unlimited - hash all files)
  # - Useful for focusing on small files only
  # - Example: 104857600 (100 MB) to only hash files under 100MB
  max_file_size: 0

  # Rate limit for hashing in MB/s (0 = unlimited)
  # - Default: 200 (200 MB/s)
  # - Helps prevent disk saturation during hash scanning
  # - Adjust based on your disk speed and I/O capacity
  max_hash_rate_mbps: 200

# ===========================
# Duplicate Consolidation Configuration
# ===========================
duplicate_consolidation:
  # Enable consolidation features
  enabled: true

  # Dry-run mode: Preview changes without actually deleting files
  # - Default: true (safe mode)
  # - Set to false to allow actual file deletions (USE WITH CAUTION)
  dry_run: true

  # Require manual approval for each duplicate group before consolidation
  # - Default: false (consolidate all at once)
  # - Set to true for per-group confirmation prompts
  require_manual_approval: false

  # Verify file integrity by re-hashing before deletion
  # - Default: true (recommended for safety)
  # - Ensures the file to keep is not corrupted
  verify_before_delete: true

  # Consolidation strategy
  # Options:
  #   - "least_full_disk": Keep file on the disk with lowest usage percentage (default)
  #   - "preferred_disk": Keep file based on a predefined disk priority order
  strategy: "least_full_disk"

  # Optional: Define preferred disk order (only used if strategy = "preferred_disk")
  # Files will be kept on the first disk in this list, then second, etc.
  # preferred_disk_order:
  #   - "Cache"
  #   - "Disk 1"
  #   - "Disk 2"
  #   - "Disk 3"

